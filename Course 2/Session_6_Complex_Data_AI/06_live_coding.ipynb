{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sesiunea 6: Complex Data Sources + AI pentru Data Science\n",
    "\n",
    "## üìã Obiective\n",
    "\n",
    "**Partea 1: Complex Data Sources (45 min)**\n",
    "- Lucru cu Excel (multiple sheets)\n",
    "- Merge »ôi concatenare datasets\n",
    "- Export √Æn formate complexe\n",
    "\n",
    "**Partea 2: AI Integration (45 min)**\n",
    "- Folosirea ChatGPT pentru cod pandas\n",
    "- Folosirea Claude pentru debugging\n",
    "- Best practices »ôi limitƒÉri\n",
    "\n",
    "**Durata:** 1.5 ore\n",
    "\n",
    "**FinalizƒÉm proiectul EU-SILC - Sesiunea 6/6** üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTEA 1: COMPLEX DATA SOURCES\n",
    "\n",
    "## 1.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T11:34:24.444984Z",
     "start_time": "2025-12-02T11:34:24.172298Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"‚úÖ Libraries loaded!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries loaded!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Working with Excel - Multiple Sheets\n",
    "\n",
    "Excel files often contain multiple sheets with related data. Let's see how to work with them."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T11:34:25.234129Z",
     "start_time": "2025-12-02T11:34:24.457774Z"
    }
   },
   "source": [
    "# First, let's create an example Excel file with multiple sheets\n",
    "# (In real scenarios, you'd receive this file)\n",
    "\n",
    "# Load our clean data\n",
    "df = pd.read_csv('../datasets/eusilc_clean.csv')\n",
    "\n",
    "# Create a metadata dictionary (variable descriptions)\n",
    "metadata = pd.DataFrame({\n",
    "    'Variable': ['HB010', 'HB020', 'HB030', 'HY010', 'HY020', 'HY022', 'HY023',\n",
    "                 'HH030', 'HX050', 'HS011', 'HS021', 'HS040',\n",
    "                 'HS110', 'HS120', 'HS130', 'HS140', 'HS150', 'HS160', 'HH010'],\n",
    "    'Description': [\n",
    "        'Survey year',\n",
    "        'Country',\n",
    "        'Household ID',\n",
    "        'Total household gross income',\n",
    "        'Total disposable income from work',\n",
    "        'Income from self-employment',\n",
    "        'Income from farming',\n",
    "        'Household size',\n",
    "        'Equivalised household size',\n",
    "        'Type of dwelling',\n",
    "        'Tenure status',\n",
    "        'Number of rooms',\n",
    "        'Bath or shower in dwelling',\n",
    "        'Financial burden of housing costs',\n",
    "        'Arrears on payments',\n",
    "        'Capacity to face unexpected expenses',\n",
    "        'Capacity to afford one week holiday',\n",
    "        'Capacity to afford meal with meat/fish',\n",
    "        'Degree of urbanisation'\n",
    "    ],\n",
    "    'Type': ['int', 'str', 'int', 'float', 'float', 'float', 'float',\n",
    "             'int', 'float', 'int', 'int', 'int',\n",
    "             'int', 'int', 'int', 'int', 'int', 'int', 'int']\n",
    "})\n",
    "\n",
    "# Create summary statistics\n",
    "summary_stats = df[['HY010', 'HY020', 'HY022', 'HY023', 'HH030']].describe()\n",
    "\n",
    "print(\"‚úÖ Example data prepared\")\n",
    "print(f\"\\nMetadata: {len(metadata)} variables documented\")\n",
    "metadata.head()"
   ],
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['HY010', 'HY020', 'HY022', 'HY023', 'HH030'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 39\u001B[39m\n\u001B[32m      8\u001B[39m metadata = pd.DataFrame({\n\u001B[32m      9\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mVariable\u001B[39m\u001B[33m'\u001B[39m: [\u001B[33m'\u001B[39m\u001B[33mHB010\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mHB020\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mHB030\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mHY010\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mHY020\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mHY022\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mHY023\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m     10\u001B[39m                  \u001B[33m'\u001B[39m\u001B[33mHH030\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mHX050\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mHS011\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mHS021\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mHS040\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     35\u001B[39m              \u001B[33m'\u001B[39m\u001B[33mint\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mint\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mint\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mint\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mint\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mint\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mint\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m     36\u001B[39m })\n\u001B[32m     38\u001B[39m \u001B[38;5;66;03m# Create summary statistics\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m summary_stats = \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mHY010\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mHY020\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mHY022\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mHY023\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mHH030\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m.describe()\n\u001B[32m     41\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m‚úÖ Example data prepared\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     42\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mMetadata: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(metadata)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m variables documented\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/osce_python_course/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4119\u001B[39m, in \u001B[36mDataFrame.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   4117\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n\u001B[32m   4118\u001B[39m         key = \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[32m-> \u001B[39m\u001B[32m4119\u001B[39m     indexer = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_get_indexer_strict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcolumns\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[32m1\u001B[39m]\n\u001B[32m   4121\u001B[39m \u001B[38;5;66;03m# take() does not accept boolean indexers\u001B[39;00m\n\u001B[32m   4122\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(indexer, \u001B[33m\"\u001B[39m\u001B[33mdtype\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) == \u001B[38;5;28mbool\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/osce_python_course/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:6212\u001B[39m, in \u001B[36mIndex._get_indexer_strict\u001B[39m\u001B[34m(self, key, axis_name)\u001B[39m\n\u001B[32m   6209\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   6210\u001B[39m     keyarr, indexer, new_indexer = \u001B[38;5;28mself\u001B[39m._reindex_non_unique(keyarr)\n\u001B[32m-> \u001B[39m\u001B[32m6212\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_raise_if_missing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeyarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   6214\u001B[39m keyarr = \u001B[38;5;28mself\u001B[39m.take(indexer)\n\u001B[32m   6215\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Index):\n\u001B[32m   6216\u001B[39m     \u001B[38;5;66;03m# GH 42790 - Preserve name from an Index\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/osce_python_course/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:6261\u001B[39m, in \u001B[36mIndex._raise_if_missing\u001B[39m\u001B[34m(self, key, indexer, axis_name)\u001B[39m\n\u001B[32m   6259\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m nmissing:\n\u001B[32m   6260\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m nmissing == \u001B[38;5;28mlen\u001B[39m(indexer):\n\u001B[32m-> \u001B[39m\u001B[32m6261\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mNone of [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m] are in the [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maxis_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m]\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   6263\u001B[39m     not_found = \u001B[38;5;28mlist\u001B[39m(ensure_index(key)[missing_mask.nonzero()[\u001B[32m0\u001B[39m]].unique())\n\u001B[32m   6264\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnot_found\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m not in index\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mKeyError\u001B[39m: \"None of [Index(['HY010', 'HY020', 'HY022', 'HY023', 'HH030'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Multi-Sheet Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Excel file with multiple sheets\n",
    "excel_path = '../datasets/eusilc_with_metadata.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "    # Sheet 1: First 100 rows of data (sample)\n",
    "    df.head(100).to_excel(writer, sheet_name='Data_Sample', index=False)\n",
    "    \n",
    "    # Sheet 2: Metadata\n",
    "    metadata.to_excel(writer, sheet_name='Variable_Descriptions', index=False)\n",
    "    \n",
    "    # Sheet 3: Summary statistics\n",
    "    summary_stats.to_excel(writer, sheet_name='Summary_Statistics')\n",
    "\n",
    "print(f\"‚úÖ Excel file created: {excel_path}\")\n",
    "print(\"\\nSheets created:\")\n",
    "print(\"  1. Data_Sample - First 100 households\")\n",
    "print(\"  2. Variable_Descriptions - Metadata\")\n",
    "print(\"  3. Summary_Statistics - Descriptive stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Multi-Sheet Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Read specific sheet\n",
    "df_data = pd.read_excel(excel_path, sheet_name='Data_Sample')\n",
    "print(\"Method 1: Read specific sheet\")\n",
    "print(f\"Data shape: {df_data.shape}\")\n",
    "\n",
    "# Method 2: Read all sheets into dictionary\n",
    "all_sheets = pd.read_excel(excel_path, sheet_name=None)\n",
    "print(f\"\\nMethod 2: Read all sheets\")\n",
    "print(f\"Sheets found: {list(all_sheets.keys())}\")\n",
    "\n",
    "# Method 3: Using ExcelFile for efficiency\n",
    "with pd.ExcelFile(excel_path) as xls:\n",
    "    print(f\"\\nMethod 3: ExcelFile object\")\n",
    "    print(f\"Sheet names: {xls.sheet_names}\")\n",
    "    \n",
    "    # Read specific sheet\n",
    "    metadata_df = pd.read_excel(xls, 'Variable_Descriptions')\n",
    "    print(f\"\\nMetadata loaded: {len(metadata_df)} variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Merging Datasets\n",
    "\n",
    "Often you need to combine data from multiple sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Merge main data with metadata\n",
    "# Create a small sample for demonstration\n",
    "df_sample = df.head(5)[['HB030', 'HY010', 'HH030']]\n",
    "\n",
    "print(\"Original data (sample):\")\n",
    "print(df_sample)\n",
    "print(\"\\nMetadata:\")\n",
    "print(metadata[['Variable', 'Description']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For merging, we need to reshape data to long format\n",
    "df_long = df_sample.melt(id_vars=['HB030'], var_name='Variable', value_name='Value')\n",
    "\n",
    "print(\"Data in long format:\")\n",
    "print(df_long.head(10))\n",
    "\n",
    "# Now merge with metadata\n",
    "df_with_metadata = df_long.merge(metadata[['Variable', 'Description']], \n",
    "                                   on='Variable', \n",
    "                                   how='left')\n",
    "\n",
    "print(\"\\nMerged data:\")\n",
    "print(df_with_metadata.head(10))\n",
    "\n",
    "print(\"\\n‚úÖ Merge types:\")\n",
    "print(\"  - inner: only matching rows\")\n",
    "print(\"  - left: all from left, matching from right\")\n",
    "print(\"  - right: all from right, matching from left\")\n",
    "print(\"  - outer: all rows from both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Concatenating Datasets\n",
    "\n",
    "Concatenation stacks datasets vertically or horizontally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical concatenation (stacking rows)\n",
    "df1 = df.head(3)\n",
    "df2 = df.tail(3)\n",
    "\n",
    "df_concat_vertical = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "\n",
    "print(\"Vertical concatenation (axis=0):\")\n",
    "print(f\"df1 shape: {df1.shape}\")\n",
    "print(f\"df2 shape: {df2.shape}\")\n",
    "print(f\"Result shape: {df_concat_vertical.shape}\")\n",
    "print(f\"\\nCombined data:\")\n",
    "print(df_concat_vertical[['HB030', 'HY010']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal concatenation (adding columns)\n",
    "df_income = df[['HB030', 'HY010', 'HY020']].head(5)\n",
    "df_housing = df[['HB030', 'HS011', 'HS040']].head(5)\n",
    "\n",
    "# Remove HB030 from second to avoid duplication\n",
    "df_housing_no_id = df_housing.drop('HB030', axis=1)\n",
    "\n",
    "df_concat_horizontal = pd.concat([df_income, df_housing_no_id], axis=1)\n",
    "\n",
    "print(\"\\nHorizontal concatenation (axis=1):\")\n",
    "print(df_concat_horizontal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Practical Example: Create Analysis Report\n",
    "\n",
    "Let's combine everything to create a comprehensive Excel report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create income quintiles for report\n",
    "df['income_quintile'] = pd.qcut(df['HY010'], q=5, \n",
    "                                  labels=['Q1 (Poorest)', 'Q2', 'Q3', 'Q4', 'Q5 (Richest)'])\n",
    "\n",
    "# Create various analysis tables\n",
    "quintile_summary = df.groupby('income_quintile')['HY010'].agg([\n",
    "    ('Count', 'count'),\n",
    "    ('Mean', 'mean'),\n",
    "    ('Median', 'median'),\n",
    "    ('Min', 'min'),\n",
    "    ('Max', 'max')\n",
    "]).round(2)\n",
    "\n",
    "# Urban-rural comparison\n",
    "urbanization_labels = {1: 'Urban', 2: 'Intermediate', 3: 'Rural'}\n",
    "df['urbanization'] = df['HH010'].map(urbanization_labels)\n",
    "urban_summary = df.groupby('urbanization')['HY010'].agg(['count', 'mean', 'median']).round(2)\n",
    "\n",
    "# Create comprehensive Excel report\n",
    "report_path = '../datasets/eusilc_analysis_report.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(report_path, engine='openpyxl') as writer:\n",
    "    # Overview\n",
    "    overview = pd.DataFrame({\n",
    "        'Metric': ['Total Households', 'Variables', 'Survey Year', 'Country'],\n",
    "        'Value': [len(df), len(df.columns), 2013, 'Estonia']\n",
    "    })\n",
    "    overview.to_excel(writer, sheet_name='Overview', index=False)\n",
    "    \n",
    "    # Income quintiles\n",
    "    quintile_summary.to_excel(writer, sheet_name='Income_Quintiles')\n",
    "    \n",
    "    # Urban-Rural\n",
    "    urban_summary.to_excel(writer, sheet_name='Urban_Rural_Comparison')\n",
    "    \n",
    "    # Variable metadata\n",
    "    metadata.to_excel(writer, sheet_name='Metadata', index=False)\n",
    "    \n",
    "    # Full summary statistics\n",
    "    df.describe().to_excel(writer, sheet_name='Summary_Statistics')\n",
    "\n",
    "print(f\"‚úÖ Comprehensive report created: {report_path}\")\n",
    "print(\"\\nReport contains:\")\n",
    "print(\"  1. Overview - Dataset summary\")\n",
    "print(\"  2. Income_Quintiles - Q1-Q5 analysis\")\n",
    "print(\"  3. Urban_Rural_Comparison - Geographic differences\")\n",
    "print(\"  4. Metadata - Variable descriptions\")\n",
    "print(\"  5. Summary_Statistics - Descriptive stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTEA 2: AI PENTRU DATA SCIENCE\n",
    "\n",
    "## 2.1 Introducere: De ce AI pentru Data Science?\n",
    "\n",
    "**AI tools (ChatGPT, Claude) pot ajuta la:**\n",
    "- ‚úÖ Generare cod pandas rapid\n",
    "- ‚úÖ Debugging (gƒÉsirea erorilor)\n",
    "- ‚úÖ Explicare cod complex\n",
    "- ‚úÖ Sugestii de analizƒÉ\n",
    "- ‚úÖ Documentare cod\n",
    "\n",
    "**Dar ATEN»öIE:**\n",
    "- ‚ö†Ô∏è AI face erori (\"hallucinations\")\n",
    "- ‚ö†Ô∏è Trebuie sƒÉ verifica»õi codul\n",
    "- ‚ö†Ô∏è Trebuie sƒÉ √Æn»õelege»õi ce face\n",
    "- ‚ö†Ô∏è Nu √Æntotdeauna e cel mai eficient cod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 ChatGPT pentru Generare Cod\n",
    "\n",
    "### Exemplu 1: Cerere simplƒÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT pentru ChatGPT:\n",
    "# \"Write pandas code to calculate income quintiles from a column named 'HY010'\"\n",
    "\n",
    "# RƒÇSPUNS ChatGPT (exemplu):\n",
    "\"\"\"\n",
    "df['income_quintile'] = pd.qcut(df['HY010'], q=5, labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])\n",
    "\"\"\"\n",
    "\n",
    "# VerificƒÉm:\n",
    "print(\"‚úÖ Cod generat de AI:\")\n",
    "print(\"df['income_quintile'] = pd.qcut(df['HY010'], q=5, labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])\")\n",
    "print(\"\\nüí° This is correct and works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplu 2: Cerere complexƒÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT pentru ChatGPT:\n",
    "\"\"\"\n",
    "I have a pandas DataFrame with columns HY010 (income), HH030 (household size), and HH010 (urbanization: 1=urban, 2=intermediate, 3=rural).\n",
    "Write code to:\n",
    "1. Create income quintiles\n",
    "2. Calculate mean income per quintile and urbanization category\n",
    "3. Create a pivot table showing this\n",
    "\"\"\"\n",
    "\n",
    "# RƒÇSPUNS ChatGPT (exemplu):\n",
    "print(\"‚úÖ AI-generated code:\")\n",
    "print(\"\"\"\n",
    "# Create quintiles\n",
    "df['quintile'] = pd.qcut(df['HY010'], q=5, labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])\n",
    "\n",
    "# Create pivot table\n",
    "pivot = df.pivot_table(\n",
    "    values='HY010',\n",
    "    index='quintile',\n",
    "    columns='HH010',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "print(pivot)\n",
    "\"\"\")\n",
    "\n",
    "# Let's test it:\n",
    "df_test = df.copy()\n",
    "df_test['quintile'] = pd.qcut(df_test['HY010'], q=5, labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])\n",
    "\n",
    "pivot = df_test.pivot_table(\n",
    "    values='HY010',\n",
    "    index='quintile',\n",
    "    columns='HH010',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "print(\"\\nüí° AI code works! Pivot table:\")\n",
    "print(pivot.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Claude pentru Debugging\n",
    "\n",
    "### Exemplu: Error fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cod cu eroare (intentional)\n",
    "print(\"‚ùå Cod cu eroare:\")\n",
    "print(\"\"\"\n",
    "# Acest cod va da eroare:\n",
    "result = df.groupby('income_quintile')['HY010'].agg(['mean', 'median', 'std'])\n",
    "result.sort_values('mean', ascending=False)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è ERROR: 'mean' is not in index\")\n",
    "print(\"\\nPROMPT pentru Claude:\")\n",
    "print(\"\"\"\n",
    "I'm getting this error:\n",
    "'mean' is not in index\n",
    "\n",
    "My code:\n",
    "result = df.groupby('income_quintile')['HY010'].agg(['mean', 'median', 'std'])\n",
    "result.sort_values('mean', ascending=False)\n",
    "\n",
    "How do I fix it?\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úÖ Claude's response:\")\n",
    "print(\"\"\"\n",
    "The issue is that 'mean' is a column name in your result, not an index.\n",
    "You need to use brackets or specify axis:\n",
    "\n",
    "result.sort_values(('mean',), ascending=False)\n",
    "# OR\n",
    "result.sort_values('mean', ascending=False, axis=0)\n",
    "# OR better:\n",
    "result.sort_values(by='mean', ascending=False)\n",
    "\"\"\")\n",
    "\n",
    "# Test the fix:\n",
    "result = df.groupby('income_quintile')['HY010'].agg(['mean', 'median', 'std'])\n",
    "result_sorted = result.sort_values(by='mean', ascending=False)\n",
    "print(\"\\nüí° Fixed code works:\")\n",
    "print(result_sorted.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Best Practices pentru AI Usage\n",
    "\n",
    "### ‚úÖ DO (Face»õi):\n",
    "\n",
    "1. **Use AI for starting points**\n",
    "   - \"Write a function to calculate Gini coefficient\"\n",
    "   - \"Create a heatmap showing correlation between variables\"\n",
    "\n",
    "2. **Use AI for explaining code**\n",
    "   - \"Explain what this code does: [paste code]\"\n",
    "   - \"What does pd.qcut() do differently from pd.cut()?\"\n",
    "\n",
    "3. **Use AI for documentation**\n",
    "   - \"Write a docstring for this function\"\n",
    "   - \"Add comments to this code\"\n",
    "\n",
    "4. **Use AI for alternatives**\n",
    "   - \"Is there a faster way to do this?\"\n",
    "   - \"Can this be done with vectorization instead of a loop?\"\n",
    "\n",
    "### ‚ùå DON'T (Nu face»õi):\n",
    "\n",
    "1. **Don't trust blindly**\n",
    "   - Always test AI code\n",
    "   - Verify statistical claims\n",
    "   - Check for errors\n",
    "\n",
    "2. **Don't use for critical decisions without validation**\n",
    "   - \"Is this statistically significant?\" ‚Üí Verify yourself\n",
    "   - \"What's the p-value?\" ‚Üí Calculate and check\n",
    "\n",
    "3. **Don't copy-paste without understanding**\n",
    "   - Read the code line by line\n",
    "   - Understand each function\n",
    "   - Know why it works\n",
    "\n",
    "4. **Don't rely on AI for data cleaning decisions**\n",
    "   - \"Should I drop or impute?\" ‚Üí Depends on YOUR data and context\n",
    "   - AI doesn't know your domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Practical Example: Using AI to Enhance Our Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT pentru ChatGPT:\n",
    "\"\"\"\n",
    "I have EU-SILC income data with quintiles already calculated.\n",
    "Write a function that generates a summary report including:\n",
    "- Income share per quintile (% of total income)\n",
    "- Q5/Q1 ratio\n",
    "- Mean and median per quintile\n",
    "\"\"\"\n",
    "\n",
    "# AI-generated function:\n",
    "def generate_inequality_report(df, income_col='HY010', quintile_col='income_quintile'):\n",
    "    \"\"\"\n",
    "    Generate inequality analysis report.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Data with income and quintile columns\n",
    "    income_col : str\n",
    "        Name of income column\n",
    "    quintile_col : str\n",
    "        Name of quintile column\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    report : dict\n",
    "        Dictionary with inequality metrics\n",
    "    \"\"\"\n",
    "    # Calculate total income per quintile\n",
    "    total_by_quintile = df.groupby(quintile_col)[income_col].sum()\n",
    "    total_income = df[income_col].sum()\n",
    "    \n",
    "    # Income share\n",
    "    income_share = (total_by_quintile / total_income * 100).round(2)\n",
    "    \n",
    "    # Mean and median\n",
    "    stats = df.groupby(quintile_col)[income_col].agg(['mean', 'median']).round(2)\n",
    "    \n",
    "    # Q5/Q1 ratio\n",
    "    q5_mean = stats.loc['Q5 (Richest)', 'mean']\n",
    "    q1_mean = stats.loc['Q1 (Poorest)', 'mean']\n",
    "    ratio = round(q5_mean / q1_mean, 2)\n",
    "    \n",
    "    report = {\n",
    "        'income_share': income_share,\n",
    "        'statistics': stats,\n",
    "        'q5_q1_ratio': ratio\n",
    "    }\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Test the function\n",
    "report = generate_inequality_report(df)\n",
    "\n",
    "print(\"üìä INEQUALITY REPORT (AI-generated function)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nIncome Share (% of total):\")\n",
    "print(report['income_share'])\n",
    "print(\"\\nStatistics per Quintile:\")\n",
    "print(report['statistics'])\n",
    "print(f\"\\nQ5/Q1 Ratio: {report['q5_q1_ratio']}x\")\n",
    "print(\"\\nüí° AI helped create this analysis function in seconds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 LimitƒÉri »ôi Precau»õii\n",
    "\n",
    "### Exemple de erori AI:\n",
    "\n",
    "**1. Hallucinations (func»õii inexistente):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå AI might suggest:\n",
    "# df.calculate_gini()  # AceastƒÉ func»õie NU EXISTƒÇ √Æn pandas!\n",
    "\n",
    "# ‚úÖ Trebuie sƒÉ implementa»õi voi:\n",
    "def gini_coefficient(x):\n",
    "    \"\"\"Calculate Gini coefficient (correctly)\"\"\"\n",
    "    # Implementation here\n",
    "    pass\n",
    "\n",
    "print(\"‚ö†Ô∏è AI sometimes invents functions that don't exist!\")\n",
    "print(\"Always check pandas documentation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Outdated syntax:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå AI might suggest old pandas syntax:\n",
    "# df.ix[0]  # Deprecated!\n",
    "\n",
    "# ‚úÖ Modern pandas:\n",
    "# df.iloc[0]  # Correct\n",
    "\n",
    "print(\"‚ö†Ô∏è AI training data might be old.\")\n",
    "print(\"Check pandas version and use current syntax.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Statistical errors:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è AI might give wrong statistical advice:\n",
    "# \"Use mean to impute because it's unbiased\"\n",
    "# ‚Üí Actually, median is often better for skewed distributions!\n",
    "\n",
    "print(\"‚ö†Ô∏è Verify statistical claims!\")\n",
    "print(\"AI doesn't understand your data distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recapitulare Sesiunea 6\n",
    "\n",
    "### ‚úÖ PARTEA 1: Complex Data\n",
    "- Excel multi-sheet: create, read, manipulate\n",
    "- Merge: combining datasets by key\n",
    "- Concat: stacking vertically/horizontally\n",
    "- Export professional reports\n",
    "\n",
    "### ‚úÖ PARTEA 2: AI Integration\n",
    "- ChatGPT for code generation\n",
    "- Claude for debugging\n",
    "- Best practices (DO/DON'T)\n",
    "- Limitations and verification\n",
    "\n",
    "### üéì Key Takeaways:\n",
    "\n",
    "**Complex Data:**\n",
    "1. Excel is powerful for reports (multiple sheets)\n",
    "2. Merge when you have related data (common key)\n",
    "3. Concat when you have similar structure\n",
    "4. Always validate after combining!\n",
    "\n",
    "**AI Usage:**\n",
    "1. ‚úÖ AI is a TOOL, not a replacement for thinking\n",
    "2. ‚úÖ Use for starting points and efficiency\n",
    "3. ‚úÖ Always verify and understand\n",
    "4. ‚úÖ Learn from AI code, don't just copy\n",
    "5. ‚ùå Never trust blindly\n",
    "6. ‚ùå Don't use for critical decisions without validation\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ CURS 2 COMPLET!\n",
    "\n",
    "**Ce a»õi √ÆnvƒÉ»õat √Æn 6 sesiuni:**\n",
    "\n",
    "1. **Sesiunea 1:** Pandas basics ‚Üí DataFrames\n",
    "2. **Sesiunea 2:** Data exploration ‚Üí identify problems\n",
    "3. **Sesiunea 3:** Data cleaning ‚Üí create clean dataset\n",
    "4. **Sesiunea 4:** EDA ‚Üí understand distributions, inequality\n",
    "5. **Sesiunea 5:** Visualization ‚Üí professional charts\n",
    "6. **Sesiunea 6:** Advanced ‚Üí Excel, merge, AI\n",
    "\n",
    "**Skills acquired:**\n",
    "- ‚úÖ pandas mastery\n",
    "- ‚úÖ Data cleaning strategies\n",
    "- ‚úÖ Statistical analysis\n",
    "- ‚úÖ Professional visualization\n",
    "- ‚úÖ Complex data handling\n",
    "- ‚úÖ AI-assisted coding\n",
    "\n",
    "**You can now:**\n",
    "- üìä Analyze real survey data (EU-SILC, BNS, etc.)\n",
    "- üßπ Clean messy data professionally\n",
    "- üìà Create income inequality analyses\n",
    "- üìâ Build dashboards and reports\n",
    "- ü§ñ Use AI to boost productivity\n",
    "- üíº Work as Data Analyst!\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Resources pentru continuare:\n",
    "\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [Kaggle Learn](https://www.kaggle.com/learn)\n",
    "- [Real Python](https://realpython.com/)\n",
    "- [ChatGPT](https://chat.openai.com/)\n",
    "- [Claude](https://claude.ai/)\n",
    "\n",
    "### üöÄ Next steps:\n",
    "\n",
    "1. **Complete Homework 2** - practice everything\n",
    "2. **Find your own dataset** - apply skills\n",
    "3. **Build portfolio** - GitHub, showcase work\n",
    "4. **Keep learning** - machine learning, time series, GIS\n",
    "5. **Join community** - Python Moldova, data science groups\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations on completing Course 2! You're now a data scientist! üéâ**\n",
    "\n",
    "*Happy analyzing! üìäüêçüöÄ*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
